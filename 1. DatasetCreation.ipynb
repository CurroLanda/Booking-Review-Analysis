{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d894fb89",
      "metadata": {},
      "source": [
        "- Amaia Rodríguez-Sierra Aguirrebeña _100472844_\n",
        "- Lucía de Frutos Martín _100475960_\n",
        "- Francisco Landa Ortega _100483174_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c2cf21e",
      "metadata": {
        "id": "0c2cf21e"
      },
      "source": [
        "# Dataset Creation: Extracting Hotel Reviews from Booking.com\n",
        "This notebook outlines the procedure used to create a custom dataset of hotel reviews. The reviews are collected from Booking.com using a structured web scraping pipeline. The dataset will later be used for various NLP and machine learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30c9386",
      "metadata": {
        "id": "e30c9386"
      },
      "source": [
        "### 1. Import and Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "WoozHQeIJxsg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "WoozHQeIJxsg",
        "outputId": "25cd9b59-e88f-4d00-c779-db2a29ef2a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Using cached googletrans-4.0.0rc1-py3-none-any.whl\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Using cached httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in c:\\users\\34684\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in c:\\users\\34684\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Using cached httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "Using cached httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "Using cached h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "Using cached rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Using cached hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "Using cached h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "Using cached hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Using cached hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.2\n",
            "    Uninstalling httpcore-1.0.2:\n",
            "      Successfully uninstalled httpcore-1.0.2\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: googletrans\n",
            "    Found existing installation: googletrans 4.0.2\n",
            "    Uninstalling googletrans-4.0.2:\n",
            "      Successfully uninstalled googletrans-4.0.2\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f4b188",
      "metadata": {
        "id": "15f4b188"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import io\n",
        "from lxml import etree\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from googletrans import Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae104833",
      "metadata": {
        "id": "ae104833"
      },
      "source": [
        "### 2. Main Workflow\n",
        "The main workflow includes steps for downloading hotel URLs, extracting user reviews, translating non-English reviews, and compiling the data into a structured format."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0965bd78",
      "metadata": {
        "id": "0965bd78"
      },
      "source": [
        "*Do not execute the next cell, it takes hours to complete*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c19fa47",
      "metadata": {
        "id": "4c19fa47"
      },
      "outputs": [],
      "source": [
        "# Function to extract reviews, hotel name, country, and rating info from a Booking hotel review page\n",
        "def fetch_reviews_with_scores(url):\n",
        "    try:\n",
        "        headers = {'Accept-Language': 'en'}\n",
        "        # Timeout added to avoid an error and full stop if url doesnt work\n",
        "        response = requests.get(url,headers=headers, timeout=30)\n",
        "        response.encoding = 'utf-8'\n",
        "\n",
        "        # If the request fails (non-200 status), skip this URL\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve {url}\")\n",
        "            return None\n",
        "\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the block containing hotel metadata\n",
        "        info_block = soup.find('div', class_='standalone_reviews_hotel_info')\n",
        "\n",
        "        # Extract hotel name\n",
        "        name_tag = info_block.find('a', class_='standalone_header_hotel_link') if info_block else None\n",
        "        name = name_tag.get_text(strip=True) if name_tag else np.nan\n",
        "\n",
        "        # Extract country\n",
        "        country_tag = info_block.find('a', class_='hotel_address_country') if info_block else None\n",
        "        country = country_tag.get_text(strip=True) if country_tag else np.nan\n",
        "\n",
        "        # Find individual review blocks\n",
        "        review_blocks = soup.find_all('li', itemprop='review')\n",
        "        reviews_list = []\n",
        "\n",
        "        for block in review_blocks:\n",
        "            # Extract review text\n",
        "            review_tag = block.find('span', itemprop='reviewBody')\n",
        "            review_text = review_tag.get_text(strip=True) if review_tag else np.nan\n",
        "\n",
        "            # Extract review score\n",
        "            score_tag = block.find('span', class_='review-score-badge')\n",
        "            score = score_tag.get_text(strip=True) if score_tag else np.nan\n",
        "\n",
        "            # Save extracted data as a dictionary\n",
        "            reviews_list.append({'Country': country, 'Name': name, 'Review': review_text, 'Rating': score, 'AvgRating': avg_score})\n",
        "\n",
        "        # Convert list to DataFrame\n",
        "        result = pd.DataFrame(reviews_list)\n",
        "        return result\n",
        "\n",
        "    # Handle request timeouts\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"Timeout occurred for URL: {url}. Skipping this URL.\")\n",
        "        return None  # Skip on timeout\n",
        "\n",
        "    # Handle any other type of exception\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred for URL {url}: {e}. Skipping this URL.\")\n",
        "        return None  # Skip on error\n",
        "\n",
        "# Get the sitemap index provided at booking.com/robots.txt\n",
        "index_url = \"https://www.booking.com/sitembk-hotel-review-index.xml\"\n",
        "response = requests.get(index_url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# Parse the sitemap XML to extract all .gz sitemap file links\n",
        "root = etree.fromstring(response.content)\n",
        "ns = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "gz_links = root.xpath(\"//ns:sitemap/ns:loc/text()\", namespaces=ns)\n",
        "\n",
        "collected_urls = []\n",
        "reviews = pd.DataFrame(columns=['Country', 'Name', 'Review', 'Rating', 'AvgRating'])\n",
        "\n",
        "# Loop through the .gz sitemap files\n",
        "for gz_url in gz_links:\n",
        "    print(f\"Downloading: {gz_url}\")\n",
        "\n",
        "    # Download and decompress the .gz sitemap file\n",
        "    gz_response = requests.get(gz_url)\n",
        "    gz_response.raise_for_status()\n",
        "    with gzip.open(io.BytesIO(gz_response.content), 'rb') as f:\n",
        "        xml_data = f.read()\n",
        "\n",
        "    # Parse the decompressed XML to extract individual hotel review page URLs\n",
        "    sub_root = etree.fromstring(xml_data)\n",
        "    urls = sub_root.xpath(\"//ns:url/ns:loc/text()\", namespaces=ns)\n",
        "    collected_urls.extend(urls)\n",
        "\n",
        "    # For each hotel review URL, extract and store review data\n",
        "    for i in collected_urls:\n",
        "        new_review = fetch_reviews_with_scores(i)\n",
        "        if new_review is not None:\n",
        "            # Append the new reviews to the main DataFrame\n",
        "            reviews = pd.concat([reviews, new_review], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y9dPpHlvHD3M",
      "metadata": {
        "id": "y9dPpHlvHD3M"
      },
      "source": [
        "### 3. Initial Clean and Preview of the Dataset\n",
        "Since a large number of reviews have been collected, it's considered safe to drop any rows that contain missing (NaN) values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1eb0a1",
      "metadata": {
        "id": "9b1eb0a1"
      },
      "outputs": [],
      "source": [
        "# Drop rows with any missing values\n",
        "new_df = reviews.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "phb3R4FBLYeT",
      "metadata": {
        "id": "phb3R4FBLYeT"
      },
      "outputs": [],
      "source": [
        "# Initialize the translator\n",
        "translator = Translator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214ac269",
      "metadata": {
        "id": "214ac269"
      },
      "outputs": [],
      "source": [
        "# Function to translate country names\n",
        "def translate_country_name(text):\n",
        "    try:\n",
        "        # Automatically detect the source language\n",
        "        return translator.translate(str(text), src=\"auto\", dest=\"en\").text\n",
        "    except:\n",
        "        # If translation fails, return the original text\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccbf1927",
      "metadata": {
        "id": "ccbf1927"
      },
      "outputs": [],
      "source": [
        "# Translate each unique country name\n",
        "unique_countries = new_df['Country'].unique()\n",
        "translations = {country: translate_country_name(country) for country in unique_countries}\n",
        "\n",
        "# Map translated country names back to the DataFrame\n",
        "new_df['New Country'] = new_df['Country'].map(translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90319fc5",
      "metadata": {
        "id": "90319fc5",
        "outputId": "406e844c-bf73-4f0d-82f4-e524eaf1c261"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Name</th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "      <th>AvgRating</th>\n",
              "      <th>New Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>تركيا</td>\n",
              "      <td>The Hera Premium Hotels</td>\n",
              "      <td>الخدمات بعيده</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>Türkiye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>تركيا</td>\n",
              "      <td>The Hera Premium Hotels</td>\n",
              "      <td>لم يصلح المكيف</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>Türkiye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>تركيا</td>\n",
              "      <td>The Hera Premium Hotels</td>\n",
              "      <td>-الافطار كان محدودا وغير ساخن \\n-ايضا بعد المس...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>Türkiye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>تركيا</td>\n",
              "      <td>The Hera Premium Hotels</td>\n",
              "      <td>تعامل الموظفين سيء: 1/ طلبت 5 قوارير ماء وقال ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>Türkiye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>تركيا</td>\n",
              "      <td>The Hera Premium Hotels</td>\n",
              "      <td>الفطور لم يكن المتوقع</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>Türkiye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262594</th>\n",
              "      <td>مصر</td>\n",
              "      <td>Lacasa Residence</td>\n",
              "      <td>فندق هادي بصلح للعائلات لكن لاسف لا توجد لوحة ...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>Egypt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262595</th>\n",
              "      <td>مصر</td>\n",
              "      <td>Lacasa Residence</td>\n",
              "      <td>مكان جميل وطاقم عمل اجمل وخصوصا الاستاذة ريحان...</td>\n",
              "      <td>10</td>\n",
              "      <td>9.5</td>\n",
              "      <td>Egypt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262596</th>\n",
              "      <td>مصر</td>\n",
              "      <td>Lacasa Residence</td>\n",
              "      <td>الشقق نظيفه. والعاملين عليها متعاونين جدا</td>\n",
              "      <td>10</td>\n",
              "      <td>9.5</td>\n",
              "      <td>Egypt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262597</th>\n",
              "      <td>مصر</td>\n",
              "      <td>Lacasa Residence</td>\n",
              "      <td>موقعه وغرفه صغيره جداً</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>Egypt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262598</th>\n",
              "      <td>مصر</td>\n",
              "      <td>Lacasa Residence</td>\n",
              "      <td>نظافة</td>\n",
              "      <td>10</td>\n",
              "      <td>9.5</td>\n",
              "      <td>Egypt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>964803 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Country                     Name  \\\n",
              "0         تركيا  The Hera Premium Hotels   \n",
              "1         تركيا  The Hera Premium Hotels   \n",
              "2         تركيا  The Hera Premium Hotels   \n",
              "3         تركيا  The Hera Premium Hotels   \n",
              "4         تركيا  The Hera Premium Hotels   \n",
              "...         ...                      ...   \n",
              "1262594     مصر         Lacasa Residence   \n",
              "1262595     مصر         Lacasa Residence   \n",
              "1262596     مصر         Lacasa Residence   \n",
              "1262597     مصر         Lacasa Residence   \n",
              "1262598     مصر         Lacasa Residence   \n",
              "\n",
              "                                                    Review Rating AvgRating  \\\n",
              "0                                            الخدمات بعيده    5.0       5.7   \n",
              "1                                           لم يصلح المكيف    7.0       5.7   \n",
              "2        -الافطار كان محدودا وغير ساخن \\n-ايضا بعد المس...    7.0       5.7   \n",
              "3        تعامل الموظفين سيء: 1/ طلبت 5 قوارير ماء وقال ...    5.0       5.7   \n",
              "4                                    الفطور لم يكن المتوقع    7.0       5.7   \n",
              "...                                                    ...    ...       ...   \n",
              "1262594  فندق هادي بصلح للعائلات لكن لاسف لا توجد لوحة ...    8.0       9.5   \n",
              "1262595  مكان جميل وطاقم عمل اجمل وخصوصا الاستاذة ريحان...     10       9.5   \n",
              "1262596          الشقق نظيفه. والعاملين عليها متعاونين جدا     10       9.5   \n",
              "1262597                             موقعه وغرفه صغيره جداً    8.0       9.5   \n",
              "1262598                                              نظافة     10       9.5   \n",
              "\n",
              "        New Country  \n",
              "0           Türkiye  \n",
              "1           Türkiye  \n",
              "2           Türkiye  \n",
              "3           Türkiye  \n",
              "4           Türkiye  \n",
              "...             ...  \n",
              "1262594       Egypt  \n",
              "1262595       Egypt  \n",
              "1262596       Egypt  \n",
              "1262597       Egypt  \n",
              "1262598       Egypt  \n",
              "\n",
              "[964803 rows x 6 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb62da9",
      "metadata": {
        "id": "2fb62da9"
      },
      "outputs": [],
      "source": [
        "# Save intermediate files to avoid data loss\n",
        "new_df.to_excel('./BookingReviews.xlsx', sheet_name='Reviews', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qqnvKm8RIEgw",
      "metadata": {
        "id": "qqnvKm8RIEgw"
      },
      "source": [
        "### 4. Translate Arabic Reviews to English\n",
        "The googletrans library (a Python API for Google Translate) is used to convert Arabic text into English. The translation step will loop through all Arabic reviews and store the English equivalents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MMIOiOXwKRdz",
      "metadata": {
        "id": "MMIOiOXwKRdz"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the first 73000 rows of reviews\n",
        "df = new_df.iloc[:73000].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiS6uqoYKYt4",
      "metadata": {
        "id": "qiS6uqoYKYt4"
      },
      "outputs": [],
      "source": [
        "# Check if the text contains Arabic characters\n",
        "def is_arabic(text):\n",
        "    return bool(re.search(r'[\\u0600-\\u06FF]', str(text)))\n",
        "\n",
        "# Translate Arabic text to English\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        return translator.translate(str(text), src='ar', dest='en').text\n",
        "    except:\n",
        "        # If translation fails, return the original text\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C-cjNxxKIDgc",
      "metadata": {
        "id": "C-cjNxxKIDgc"
      },
      "outputs": [],
      "source": [
        "translated_rows = []\n",
        "for idx, row in df.iterrows():\n",
        "    # Translate the review text from Arabic to English\n",
        "    translated_review = translate_text(row['Review'])\n",
        "\n",
        "    # Check if the name is in Arabic and translate it if needed\n",
        "    name = row['Name']\n",
        "    translated_name = translate_text(name) if is_arabic(name) else name\n",
        "\n",
        "    # Create a new row dictionary with translated content\n",
        "    translated_rows.append({\n",
        "        'Name': translated_name,\n",
        "        'Review': translated_review,\n",
        "        'Rating': row['Rating'],        # Original numerical rating\n",
        "        'AvgRating': row['AvgRating'],  # Original average rating\n",
        "        'Country': row['Country']       # Original country\n",
        "    })\n",
        "\n",
        "    # Print progress every 1000 rows\n",
        "    if (idx + 1) % 1000 == 0:\n",
        "        print(f\"{idx + 1} rows translated...\")\n",
        "\n",
        "# Convert the list of translated rows into a new DataFrame\n",
        "translated_df = pd.DataFrame(translated_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4t6qAB7zKcxU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t6qAB7zKcxU",
        "outputId": "dedb846a-6ae8-4146-80cd-6b5c3d9899d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation completed.\n"
          ]
        }
      ],
      "source": [
        "# Save the translated reviews into a new CSV file\n",
        "translated_df.to_csv('BookingReviews_Translated.csv', index=False)\n",
        "print(\"Translation completed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
